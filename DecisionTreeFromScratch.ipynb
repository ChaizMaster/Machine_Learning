{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#To ensure all the plots are displayed in the notebook\n",
    "%matplotlib inline \n",
    "import random\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset and convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So we are formating the dataset to suit our api format which requires the class of flowers \n",
    "#to be called labels to signify that it is the output to training data points and also we\n",
    "#converted data into dataframe for ease of operations with help of pandas\n",
    "# from sklearn import datasets\n",
    "# iris=datasets.load_iris()\n",
    "# df= pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "#                  columns= iris['feature_names'] + ['target'])\n",
    "# df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
    "# df=df.rename(columns={\"species\":\"label\"})\n",
    "# df.drop(\"target\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We write our own train_test split function without using sklearn\n",
    "def train_test_split(df,test_size):\n",
    "    if isinstance(test_size,float):\n",
    "        #if user inputs a proportion as test_size then convert to integer\n",
    "        test_size=round(test_size*len(df))\n",
    "    #convert the row indices to a vector for random func to use\n",
    "    indices=df.index.tolist()\n",
    "    #fetch random test indices to construct test_df\n",
    "    test_indices=random.sample(population=indices,k=test_size)\n",
    "    test_df=df.loc[test_indices]\n",
    "    train_df=df.drop(test_indices)\n",
    "    return train_df,test_df\n",
    "#to check whether implementation is as intended\n",
    "# random.seed(0)\n",
    "# train_df,test_df=train_test_split(df,test_size=20)\n",
    "# print(len(test_df),len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will operate on numpy arrays instead of pandas as numpy allows very fast operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) is Data pure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data should be a numpy 2-D array and this function returns a boolean\n",
    "def isDataPure(data):\n",
    "    label_col=data[:,-1]\n",
    "    unique_classes=np.unique(label_col)\n",
    "    if len(unique_classes)==1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "# test \n",
    "# isDataPure(train_df.values)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Classify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we want to have a classify function which will be used in two cases if there \n",
    "# are no features left then it will return majority, else if we reach a pure class\n",
    "def classifier(data):\n",
    "    label_col=data[:,-1]\n",
    "#   to fetch the unique classes we have in our data sets and the counts of their occurence\n",
    "    unique_classes,unique_classes_counts=np.unique(label_col,return_counts=True)\n",
    "    index_of_majority_class=unique_classes_counts.argmax()\n",
    "    classification=unique_classes[index_of_majority_class]\n",
    "    return classification\n",
    "# test\n",
    "# classifier(train_df[train_df[\"petal width (cm)\"]<0.8].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Choose which feature to split on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we have continious dataset so we will have to choose a boundary to split on\n",
    "#so for that we will write a function which will take in the data choose the best feature\n",
    "#and returns a DICTIONARY, with key as the feature chosen and value as an array of \n",
    "#potential boundaries we can choose from.\n",
    "def getPotentialSplits(data):\n",
    "    potentialSplits={}\n",
    "    _,nCols=data.shape\n",
    "    #fetch all cols except last one(which is label column)\n",
    "    for col in range(nCols-1):\n",
    "        values=data[:,col]\n",
    "        unique_values=np.unique(values)\n",
    "        #this will return all the unique values in sorted order then around which we can\n",
    "        #calculate the boundaries\n",
    "        potentialSplits[col]=unique_values\n",
    "    return potentialSplits\n",
    "# potentialSplits=getPotentialSplits(train_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So we have to split data after selecting the feature on which we would split\n",
    "#we have to pass in the feature and the value of the boundary we picked up from \n",
    "#potential splits and chose to split on. The function will return two 2-d numpy arrays\n",
    "#with one representing all the values to the left of split boundary(smaller) and the other to the\n",
    "#right(bigger)\n",
    "def splitData(data,splitColumn,splitValue):\n",
    "    splitColumnValues=data[:,splitColumn]\n",
    "    type_of_feature=feature_types[splitColumn]\n",
    "    if type_of_feature==\"continuous\":\n",
    "        data_below=data[splitColumnValues<=splitValue]\n",
    "        data_above=data[splitColumnValues>splitValue]\n",
    "    else:\n",
    "        data_below=data[splitColumnValues==splitValue]\n",
    "        data_above=data[splitColumnValues!=splitValue]\n",
    "        \n",
    "    return data_below,data_above\n",
    "# test\n",
    "# splitCol=3\n",
    "# splitVal=0.8\n",
    "# data_below,data_above=splitData(train_df.values,splitCol,splitVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) lowest overall entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateEntropy(data):\n",
    "    label_column=data[:,-1]\n",
    "    _,counts=np.unique(label_column,return_counts=True)\n",
    "    probabilities=counts/counts.sum()\n",
    "    entropy=sum(probabilities* -np.log2(probabilities))\n",
    "    return entropy\n",
    "#test\n",
    "# calculateEntropy(data_above)\n",
    "def calculateOverallEntropy(data_below,data_above):\n",
    "    num_data_points=len(data_below)+len(data_above)\n",
    "    p_data_below=len(data_below)/num_data_points\n",
    "    p_data_above=len(data_above)/num_data_points\n",
    "    overallEntropy=(p_data_below*calculateEntropy(data_below)\n",
    "                   +p_data_above*calculateEntropy(data_above))\n",
    "    splitInfo=-((len(data_below)/num_data_points)*(np.log2(len(data_below)/num_data_points)))-((len(data_above)/num_data_points)*(np.log2(len(data_above)/num_data_points)))\n",
    "    return overallEntropy,overallEntropy/splitInfo\n",
    "#test\n",
    "# calculateOverallEntropy(data_below,data_above)\n",
    "# The function that calculates the best feature to split on by looking at entropies\n",
    "# So this function calculates the entropy for all potential boundaries and selects\n",
    "# the one with the lowest overall entropy which tells us that, that feature should\n",
    "# be selected to split onx\n",
    "def determineBestSplit(data,potentialSplits):\n",
    "    overallEntropy=float('inf')\n",
    "    gain_ratio=None\n",
    "    for colIdx in potentialSplits:\n",
    "        for value in potentialSplits[colIdx]:\n",
    "            data_below,data_above=splitData(data,splitColumn=colIdx,splitValue=value)\n",
    "            currentOverallEntropy,gain_r=calculateOverallEntropy(data_below,data_above)\n",
    "            if currentOverallEntropy<=overallEntropy:\n",
    "                overallEntropy=currentOverallEntropy\n",
    "                bestSplitColumn=colIdx\n",
    "                bestSplitValue=value\n",
    "                gain_ratio=gain_r\n",
    "    return bestSplitColumn,bestSplitValue,overallEntropy,gain_ratio\n",
    "# test\n",
    "# splitCol,splitVal=determineBestSplit(train_df.values,potentialSplits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will classify a give input to the classes possible\n",
    "def classify(inp,tree):\n",
    "    question=list(tree.keys())[0]\n",
    "    feature_name,comparison_operator,value=question.split()\n",
    "    if comparison_operator==\"<=\":\n",
    "        if inp[feature_name]<=float(value):\n",
    "            answer=tree[question][0]\n",
    "        else:\n",
    "            answer=tree[question][1]\n",
    "    else:\n",
    "        if str(inp[feature_name])==value:\n",
    "            answer=tree[question][0]\n",
    "        else:\n",
    "            answer=tree[question][1]\n",
    "    #base case\n",
    "    if not isinstance(answer,dict):\n",
    "        return answer\n",
    "    #recursive call\n",
    "    else:\n",
    "        return classify(inp,answer)\n",
    "# we need a metric to test our model so we will write an accuracy function that will\n",
    "# check our predicted value against original value and then take the mean of total predictions\n",
    "# to calculate the accuracy\n",
    "def accuracy(df,tree):\n",
    "    df[\"classification\"]=df.apply(classify,axis=1,args=(tree,))\n",
    "    df[\"classification_correctness\"]=df.classification==df.label\n",
    "    accuracy=df.classification_correctness.mean()\n",
    "    return accuracy\n",
    "#This allows us to differentiate between categorical and continuous features\n",
    "def determine_type_of_feature(df):\n",
    "    \n",
    "    feature_types = []\n",
    "    n_unique_values_treshold = 15\n",
    "    for feature in df.columns:\n",
    "        if feature != \"label\":\n",
    "            unique_values = df[feature].unique()\n",
    "            example_value = unique_values[0]\n",
    "\n",
    "            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "    \n",
    "    return feature_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now we have all the ingredients to construct a decision tree algorithm\n",
    "# to represent the tree we will use graph and the key of graph would be the feature\n",
    "# and value of it can be either of the two things\n",
    "# i) it's either a class which means we reached a leaf\n",
    "# ii) or it can be a another feature on which we will split further\n",
    "# so example of tree is\n",
    "# tree={\"petal width<=0.8\":[\"Iris-setosa\"],\n",
    "#                            \"petal width\"<=1.65:[{\"petal length<=4.9\":[\"Iris-versicolor\",\n",
    "#                                                                      \"Iris-virginica\"]}],\n",
    "#                                                 \"Iris-virginica\"]}]}\n",
    "# {question:[yes,no]}--> Tree Representation\n",
    "def decisionTree(df,counter=0,level=0):\n",
    "    #so idea is to keep the api as simple as possible so we ask a df from user and covert\n",
    "    #to numpy 2-d array and then pass it on recursively     \n",
    "    #step1: data preparation\n",
    "    if counter==0:\n",
    "        global column_headers,feature_types\n",
    "        column_headers=df.columns\n",
    "        feature_types=determine_type_of_feature(df)\n",
    "        data=df.values\n",
    "    else:\n",
    "        data=df\n",
    "    #step2: base case (if data is pure or features are finished)\n",
    "    if isDataPure(data):\n",
    "        print(\"Reached Leaf Node\")\n",
    "        print(\"\")\n",
    "        classification=classifier(data)\n",
    "        return classification\n",
    "    #step3:recursive part(selecting features and splitting further)\n",
    "    counter+=1\n",
    "    #so this gets us all the potential splits and then determines the best feature \n",
    "    #to split on and then splits on basis of that feature.\n",
    "    potentialSplits=getPotentialSplits(data)\n",
    "    splitCol,splitVal,entropy,gain_ratio=determineBestSplit(data,potentialSplits)\n",
    "    print(\"\")\n",
    "    print(\"Level \",level)\n",
    "    print(\"Current Entropy is=\",entropy)\n",
    "    print(\"Splitting on feature:\",column_headers[splitCol],\"on value:\",splitVal,\"with gain ratio\",gain_ratio)\n",
    "    data_below,data_above=splitData(data,splitCol,splitVal)\n",
    "    #check for empty data\n",
    "    if len(data_below)==0 or len(data_above)==0:\n",
    "        classification=classifier(data)\n",
    "        return classification\n",
    "    #so now we have to create subtrees for splitting further which will either result\n",
    "    # in a leaf node or will form basis for further branching out of the tree.\n",
    "    feature_name=column_headers[splitCol]\n",
    "    type_of_feature=feature_types[splitCol]\n",
    "    if type_of_feature==\"continuous\":\n",
    "        question=\"{} <= {}\".format(feature_name,splitVal)\n",
    "    else:\n",
    "        question=\"{} = {}\".format(feature_name,splitVal)\n",
    "    subTree={question:[]}\n",
    "    #so to get the answers to our question we have to perform a split which will fill in\n",
    "    # the answers. This is the recursive part of the algorithm\n",
    "    yes=decisionTree(data_below,counter,level+1)\n",
    "    no=decisionTree(data_above,counter,level+1)\n",
    "    subTree[question].append(yes)\n",
    "    subTree[question].append(no)\n",
    "    return subTree\n",
    "#test\n",
    "# tree=decisionTree(train_df)\n",
    "# pprint(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Level  0\n",
      "Current Entropy is= 0.6749257854068225\n",
      "Splitting on feature: petal_width on value: 0.5 with gain ratio 0.741891817518437\n",
      "Reached Leaf Node\n",
      "\n",
      "\n",
      "Level  1\n",
      "Current Entropy is= 0.1670240204162191\n",
      "Splitting on feature: petal_length on value: 4.8 with gain ratio 0.16704238631658044\n",
      "\n",
      "Level  2\n",
      "Current Entropy is= 0.05\n",
      "Splitting on feature: petal_width on value: 1.6 with gain ratio 0.17458286045880028\n",
      "Reached Leaf Node\n",
      "\n",
      "\n",
      "Level  3\n",
      "Current Entropy is= 0.0\n",
      "Splitting on feature: sepal_width on value: 2.8 with gain ratio 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Reached Leaf Node\n",
      "\n",
      "\n",
      "Level  2\n",
      "Current Entropy is= 0.08804001157162955\n",
      "Splitting on feature: petal_width on value: 1.6 with gain ratio 0.16457808872265628\n",
      "\n",
      "Level  3\n",
      "Current Entropy is= 0.4\n",
      "Splitting on feature: petal_width on value: 1.5 with gain ratio 0.4119674083156196\n",
      "Reached Leaf Node\n",
      "\n",
      "\n",
      "Level  4\n",
      "Current Entropy is= 0.0\n",
      "Splitting on feature: petal_length on value: 5.1 with gain ratio 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Reached Leaf Node\n",
      "\n",
      "Reached Leaf Node\n",
      "\n",
      "Accuracy is:  0.8333333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Torre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in log2\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\Torre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"converted_data.csv\")\n",
    "df = df.drop(\"Id\", axis=1)\n",
    "df = df.rename(columns={\"species\": \"label\"})\n",
    "global dic\n",
    "dic={}\n",
    "i=0\n",
    "for col in df.columns[:-1]:\n",
    "    dic[i]=col\n",
    "    i+=1\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "tree = decisionTree(train_df)\n",
    "pred_accuracy=accuracy(test_df, tree)\n",
    "print(\"Accuracy is: \",pred_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
